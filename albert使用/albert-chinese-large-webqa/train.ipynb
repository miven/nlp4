{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: NVIDIA_VISIBLE_DEVICES=6,7\n",
      "env: CUDA_VISIBLE_DEVICES=6,7\n"
     ]
    }
   ],
   "source": [
    "%env NVIDIA_VISIBLE_DEVICES 6,7\n",
    "%env CUDA_VISIBLE_DEVICES 6,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/blob/master/examples/run_squad.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: run_squad.py [-h] --model_type MODEL_TYPE --model_name_or_path\n",
      "                    MODEL_NAME_OR_PATH --output_dir OUTPUT_DIR\n",
      "                    [--data_dir DATA_DIR] [--train_file TRAIN_FILE]\n",
      "                    [--predict_file PREDICT_FILE] [--config_name CONFIG_NAME]\n",
      "                    [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
      "                    [--version_2_with_negative]\n",
      "                    [--null_score_diff_threshold NULL_SCORE_DIFF_THRESHOLD]\n",
      "                    [--max_seq_length MAX_SEQ_LENGTH]\n",
      "                    [--doc_stride DOC_STRIDE]\n",
      "                    [--max_query_length MAX_QUERY_LENGTH] [--do_train]\n",
      "                    [--do_eval] [--evaluate_during_training] [--do_lower_case]\n",
      "                    [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "                    [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                    [--learning_rate LEARNING_RATE]\n",
      "                    [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                    [--weight_decay WEIGHT_DECAY]\n",
      "                    [--adam_epsilon ADAM_EPSILON]\n",
      "                    [--max_grad_norm MAX_GRAD_NORM]\n",
      "                    [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                    [--max_steps MAX_STEPS] [--warmup_steps WARMUP_STEPS]\n",
      "                    [--n_best_size N_BEST_SIZE]\n",
      "                    [--max_answer_length MAX_ANSWER_LENGTH]\n",
      "                    [--verbose_logging] [--lang_id LANG_ID]\n",
      "                    [--logging_steps LOGGING_STEPS] [--save_steps SAVE_STEPS]\n",
      "                    [--eval_all_checkpoints] [--no_cuda]\n",
      "                    [--overwrite_output_dir] [--overwrite_cache] [--seed SEED]\n",
      "                    [--local_rank LOCAL_RANK] [--fp16]\n",
      "                    [--fp16_opt_level FP16_OPT_LEVEL] [--server_ip SERVER_IP]\n",
      "                    [--server_port SERVER_PORT] [--threads THREADS]\n",
      "                    [--no_cache]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model_type MODEL_TYPE\n",
      "                        Model type selected in the list: distilbert, albert,\n",
      "                        roberta, bert, xlnet, flaubert, xlm\n",
      "  --model_name_or_path MODEL_NAME_OR_PATH\n",
      "                        Path to pre-trained model or shortcut name selected in\n",
      "                        the list: distilbert-base-uncased, distilbert-base-\n",
      "                        uncased-distilled-squad, distilbert-base-cased,\n",
      "                        distilbert-base-cased-distilled-squad, distilbert-\n",
      "                        base-german-cased, distilbert-base-multilingual-cased,\n",
      "                        distilbert-base-uncased-finetuned-sst-2-english,\n",
      "                        albert-base-v1, albert-large-v1, albert-xlarge-v1,\n",
      "                        albert-xxlarge-v1, albert-base-v2, albert-large-v2,\n",
      "                        albert-xlarge-v2, albert-xxlarge-v2, roberta-base,\n",
      "                        roberta-large, roberta-large-mnli, distilroberta-base,\n",
      "                        roberta-base-openai-detector, roberta-large-openai-\n",
      "                        detector, bert-base-uncased, bert-large-uncased, bert-\n",
      "                        base-cased, bert-large-cased, bert-base-multilingual-\n",
      "                        uncased, bert-base-multilingual-cased, bert-base-\n",
      "                        chinese, bert-base-german-cased, bert-large-uncased-\n",
      "                        whole-word-masking, bert-large-cased-whole-word-\n",
      "                        masking, bert-large-uncased-whole-word-masking-\n",
      "                        finetuned-squad, bert-large-cased-whole-word-masking-\n",
      "                        finetuned-squad, bert-base-cased-finetuned-mrpc, bert-\n",
      "                        base-german-dbmdz-cased, bert-base-german-dbmdz-\n",
      "                        uncased, bert-base-japanese, bert-base-japanese-whole-\n",
      "                        word-masking, bert-base-japanese-char, bert-base-\n",
      "                        japanese-char-whole-word-masking, bert-base-finnish-\n",
      "                        cased-v1, bert-base-finnish-uncased-v1, bert-base-\n",
      "                        dutch-cased, xlnet-base-cased, xlnet-large-cased,\n",
      "                        flaubert-small-cased, flaubert-base-uncased, flaubert-\n",
      "                        base-cased, flaubert-large-cased, xlm-mlm-en-2048,\n",
      "                        xlm-mlm-ende-1024, xlm-mlm-enfr-1024, xlm-mlm-\n",
      "                        enro-1024, xlm-mlm-tlm-xnli15-1024, xlm-mlm-\n",
      "                        xnli15-1024, xlm-clm-enfr-1024, xlm-clm-ende-1024,\n",
      "                        xlm-mlm-17-1280, xlm-mlm-100-1280\n",
      "  --output_dir OUTPUT_DIR\n",
      "                        The output directory where the model checkpoints and\n",
      "                        predictions will be written.\n",
      "  --data_dir DATA_DIR   The input data dir. Should contain the .json files for\n",
      "                        the task.If no data dir or train/predict files are\n",
      "                        specified, will run with tensorflow_datasets.\n",
      "  --train_file TRAIN_FILE\n",
      "                        The input training file. If a data dir is specified,\n",
      "                        will look for the file thereIf no data dir or\n",
      "                        train/predict files are specified, will run with\n",
      "                        tensorflow_datasets.\n",
      "  --predict_file PREDICT_FILE\n",
      "                        The input evaluation file. If a data dir is specified,\n",
      "                        will look for the file thereIf no data dir or\n",
      "                        train/predict files are specified, will run with\n",
      "                        tensorflow_datasets.\n",
      "  --config_name CONFIG_NAME\n",
      "                        Pretrained config name or path if not the same as\n",
      "                        model_name\n",
      "  --tokenizer_name TOKENIZER_NAME\n",
      "                        Pretrained tokenizer name or path if not the same as\n",
      "                        model_name\n",
      "  --cache_dir CACHE_DIR\n",
      "                        Where do you want to store the pre-trained models\n",
      "                        downloaded from s3\n",
      "  --version_2_with_negative\n",
      "                        If true, the SQuAD examples contain some that do not\n",
      "                        have an answer.\n",
      "  --null_score_diff_threshold NULL_SCORE_DIFF_THRESHOLD\n",
      "                        If null_score - best_non_null is greater than the\n",
      "                        threshold predict null.\n",
      "  --max_seq_length MAX_SEQ_LENGTH\n",
      "                        The maximum total input sequence length after\n",
      "                        WordPiece tokenization. Sequences longer than this\n",
      "                        will be truncated, and sequences shorter than this\n",
      "                        will be padded.\n",
      "  --doc_stride DOC_STRIDE\n",
      "                        When splitting up a long document into chunks, how\n",
      "                        much stride to take between chunks.\n",
      "  --max_query_length MAX_QUERY_LENGTH\n",
      "                        The maximum number of tokens for the question.\n",
      "                        Questions longer than this will be truncated to this\n",
      "                        length.\n",
      "  --do_train            Whether to run training.\n",
      "  --do_eval             Whether to run eval on the dev set.\n",
      "  --evaluate_during_training\n",
      "                        Run evaluation during training at each logging step.\n",
      "  --do_lower_case       Set this flag if you are using an uncased model.\n",
      "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n",
      "                        Batch size per GPU/CPU for training.\n",
      "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n",
      "                        Batch size per GPU/CPU for evaluation.\n",
      "  --learning_rate LEARNING_RATE\n",
      "                        The initial learning rate for Adam.\n",
      "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
      "                        Number of updates steps to accumulate before\n",
      "                        performing a backward/update pass.\n",
      "  --weight_decay WEIGHT_DECAY\n",
      "                        Weight decay if we apply some.\n",
      "  --adam_epsilon ADAM_EPSILON\n",
      "                        Epsilon for Adam optimizer.\n",
      "  --max_grad_norm MAX_GRAD_NORM\n",
      "                        Max gradient norm.\n",
      "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
      "                        Total number of training epochs to perform.\n",
      "  --max_steps MAX_STEPS\n",
      "                        If > 0: set total number of training steps to perform.\n",
      "                        Override num_train_epochs.\n",
      "  --warmup_steps WARMUP_STEPS\n",
      "                        Linear warmup over warmup_steps.\n",
      "  --n_best_size N_BEST_SIZE\n",
      "                        The total number of n-best predictions to generate in\n",
      "                        the nbest_predictions.json output file.\n",
      "  --max_answer_length MAX_ANSWER_LENGTH\n",
      "                        The maximum length of an answer that can be generated.\n",
      "                        This is needed because the start and end predictions\n",
      "                        are not conditioned on one another.\n",
      "  --verbose_logging     If true, all of the warnings related to data\n",
      "                        processing will be printed. A number of warnings are\n",
      "                        expected for a normal SQuAD evaluation.\n",
      "  --lang_id LANG_ID     language id of input for language-specific xlm models\n",
      "                        (see tokenization_xlm.PRETRAINED_INIT_CONFIGURATION)\n",
      "  --logging_steps LOGGING_STEPS\n",
      "                        Log every X updates steps.\n",
      "  --save_steps SAVE_STEPS\n",
      "                        Save checkpoint every X updates steps.\n",
      "  --eval_all_checkpoints\n",
      "                        Evaluate all checkpoints starting with the same prefix\n",
      "                        as model_name ending and ending with step number\n",
      "  --no_cuda             Whether not to use CUDA when available\n",
      "  --overwrite_output_dir\n",
      "                        Overwrite the content of the output directory\n",
      "  --overwrite_cache     Overwrite the cached training and evaluation sets\n",
      "  --seed SEED           random seed for initialization\n",
      "  --local_rank LOCAL_RANK\n",
      "                        local_rank for distributed training on gpus\n",
      "  --fp16                Whether to use 16-bit (mixed) precision (through\n",
      "                        NVIDIA apex) instead of 32-bit\n",
      "  --fp16_opt_level FP16_OPT_LEVEL\n",
      "                        For fp16: Apex AMP optimization level selected in\n",
      "                        ['O0', 'O1', 'O2', and 'O3'].See details at\n",
      "                        https://nvidia.github.io/apex/amp.html\n",
      "  --server_ip SERVER_IP\n",
      "                        Can be used for distant debugging.\n",
      "  --server_port SERVER_PORT\n",
      "                        Can be used for distant debugging.\n",
      "  --threads THREADS     multiple threads for converting example to features\n",
      "  --no_cache            whether to disable caching features\n"
     ]
    }
   ],
   "source": [
    "%run run_squad -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run run_squad \\\n",
    "    --model_type albert \\\n",
    "    --model_name_or_path ../pretrained/albert-chinese-large/ \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --evaluate_during_training \\\n",
    "    --train_file ../data/webqa_dureader_squad_train.json \\\n",
    "    --predict_file ../data/webqa_dureader_squad_eval.json \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --max_seq_length 512 \\\n",
    "    --max_query_length 50 \\\n",
    "    --max_answer_length 300 \\\n",
    "    --doc_stride 256 \\\n",
    "    --output_dir models/albert-chinese-large2 \\\n",
    "    --num_train_epochs  2 \\\n",
    "    --warmup_steps 1000 \\\n",
    "    --per_gpu_eval_batch_size 8 \\\n",
    "    --per_gpu_train_batch_size 8 \\\n",
    "    --gradient_accumulation_steps  3\\\n",
    "    --save_steps 2500 \\\n",
    "    --version_2_with_negative \\\n",
    "    --threads 24 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
